{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cf14d57-ac2c-4ecd-a200-e4d907c29333",
   "metadata": {},
   "source": [
    "# HPC intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af640881-f322-4287-a6f4-ef65e0ee91cf",
   "metadata": {},
   "source": [
    "## atools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ac4663-1aa8-4c0c-8490-0df632a54d72",
   "metadata": {},
   "source": [
    "There are many situations in which you want to run an application for (potentially many) different input parameters.  These parameters can be command line options you run your application with, or file names you provide and so on.\n",
    "\n",
    "Of course, you could submit a job for each of the instances of your problem, but that would result in many jobs.  Moreover, quite some bookkeeping would be required if some instances fail, while others succeed.  You typically don't have a convenient way to get an overview of which instances failed, and hence have to be redone.\n",
    "\n",
    "Alternatively, you could simply do all these instances looping over all the parameters.  This would result in potentially prohibitively long run times, and, more importantly, you would not be exploiting a supercomputers main feature: executing work in parallel.\n",
    "\n",
    "atools has been designed to make it easy for you to run many instances of a problem in parallel, and it takes care of the bookkeeping for you as well.  An instance of the problem that you want to compute is called a *work item* in the context of atools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3be777-a69b-47f8-a329-facbec0330f2",
   "metadata": {},
   "source": [
    "### Job script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49de7a44-8062-4bbb-bf8e-e2362f27ba17",
   "metadata": {},
   "source": [
    "The first step is to make a few modifications to your job script.  By way of example, use a script that simply calculates and displays the product of two numbers that you also used in the [tutorial on jobs](020_jobs.ipynb).\n",
    "\n",
    "```bash\n",
    "#!/usr/bin/env -S bash -l\n",
    "#SBATCH --account=lp_multiscale_physics\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=1g\n",
    "#SBATCH --time=00:05:00\n",
    "\n",
    "# actual computation, a bit boring\n",
    "for i in $(seq 1 10)\n",
    "do\n",
    "    for j in $(seq 1 10)\n",
    "    do\n",
    "        echo $(( $i * $j ))\n",
    "    done\n",
    "done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda8376c-c321-4c94-b67e-a633203f8a49",
   "metadata": {},
   "source": [
    "In this job script, you do all computations sequentially, but to speed things up, you would like to do them in parallel as independent jobs.  So you can rewrite the job script such that it only does a single multiplication.\n",
    "\n",
    "```bash\n",
    "#!/usr/bin/env -S bash -l\n",
    "#SBATCH --account=lp_multiscale_physics\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=1g\n",
    "#SBATCH --time=00:05:00\n",
    "\n",
    "# actual computation, a bit boring\n",
    "echo $(( $i * $j ))\n",
    "```\n",
    "\n",
    "The job script has been adapted to compute a single work item."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d2603b-1b9c-41cd-8e04-55ce27482b50",
   "metadata": {},
   "source": [
    "This is where atools comes in.  You can make a few more modifications to this job script to use it.  The values of `i` and `j` will be read from a Comma Separated Value file (CSV file).\n",
    "\n",
    "The first line of this file lists the names of the variables, each line after that the values that correspond to the work items.  So for this example, that would look as follows.  \n",
    "\n",
    "```\n",
    "i,j\n",
    "1,1\n",
    "1,2\n",
    "1,3\n",
    "...\n",
    "10,8\n",
    "10,9\n",
    "10,10\n",
    "```\n",
    "\n",
    "You don't have to type all that, there is a data file `data.csv` available that you can copy.  You can find it in the `021_artefacts` directory."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d44421c-e833-455a-bf02-e810952a235c",
   "metadata": {},
   "source": [
    "As it is, this script would fail since at this point the variables `i` and `j` are not defined.  You have to make sure that atools can do its magic.  For that purpose, you have to make a few more modifications to the job script.\n",
    "\n",
    "1. Load the `atools` module.\n",
    "2. Log the start of the work item.\n",
    "3. Make sure that the variables used in the script are initialized.\n",
    "4. Log the end of the work item.\n",
    "\n",
    "```bash\n",
    "#!/usr/bin/env -S bash -l\n",
    "#SBATCH --account=lp_multiscale_physics\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=1g\n",
    "#SBATCH --time=00:05:00\n",
    "\n",
    "# make sure the module system starts from a clean slate and load the atools module\n",
    "module purge\n",
    "module load atools\n",
    "\n",
    "# log the start of the work item\n",
    "alog  --state start\n",
    "\n",
    "# initialize the variables\n",
    "source <(aenv --data data.csv)\n",
    "\n",
    "# actual computation, a bit boring\n",
    "echo $(( $i * $j ))\n",
    "\n",
    "# log the end of the work item\n",
    "alog  --state end  --exit $?\n",
    "```\n",
    "\n",
    "Now your job script is fully adapted to use atools features.  It is available in the `021_artefacts` directory as `jobscript_parallel.slurm`.   Don't forget to change the credit account name to the one you have access to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142ca124-5140-4576-81b9-a336dfba0fff",
   "metadata": {},
   "source": [
    "### Job submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda2da63-b165-495b-bf6b-bced8c02a8c3",
   "metadata": {},
   "source": [
    "You can submit an atools job almost the same way as an ordinary job, except that you need to specify the `--array` option for `sbatch`.  If you know the number of work items, 100 in the `data.csv` file you are using, you can simply use `--array=1-100`.  Otherwise, atools can help you determine it easily.\n",
    "\n",
    "First, load the atools module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90a85580-e2cb-4096-bfab-44b9fd78517b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "module load atools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0c14a9-e49b-4459-b26d-dafb0a6da0d0",
   "metadata": {},
   "source": [
    "Next, submit the job as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b79bed48-009f-4ff6-a64e-e1ee7f864555",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 60666074 on cluster wice\n"
     ]
    }
   ],
   "source": [
    "sbatch  --cluster=wice  --array=$(arange --data data.csv)  jobscript_parallel.slurm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d6c0d0-043d-47b7-a29f-13afa9dad826",
   "metadata": {},
   "source": [
    "When you check the queue, you will notice that\n",
    "* when your job is not running yet, the job ID is somewhat unusual, `_[1-100]` is appended to it;\n",
    "* when your job has started to run, you will see many entries where a single number was appended to the job ID; these are the indivitual work items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a6cd489-11cb-41a9-a62d-64f5309b808d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLUSTER: wice\n",
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "  60666074_[1-100]     batch jobscrip vsc30032 PD       0:00      1 (Priority)\n",
      "          60665918 interacti sys/dash vsc30032  R      15:18      1 k28i14\n"
     ]
    }
   ],
   "source": [
    "squeue --cluster=wice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17f10981-94aa-48af-ae2f-57768b3149e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLUSTER: wice\n",
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "  60666074_[1-100]     batch jobscrip vsc30032 PD       0:00      1 (Priority)\n",
      "          60665918 interacti sys/dash vsc30032  R      15:22      1 k28i14\n"
     ]
    }
   ],
   "source": [
    "squeue --cluster=wice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a2a29d3-bfa5-4a5d-aa16-1271629e6207",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLUSTER: wice\n",
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      " 60666074_[21-100]     batch jobscrip vsc30032 PD       0:00      1 (Priority)\n",
      "        60666074_1     batch jobscrip vsc30032  R       0:02      1 m28c20n2\n",
      "        60666074_2     batch jobscrip vsc30032  R       0:02      1 s28c11n2\n",
      "        60666074_3     batch jobscrip vsc30032  R       0:02      1 n28c30n1\n",
      "        60666074_4     batch jobscrip vsc30032  R       0:02      1 n28c30n1\n",
      "        60666074_5     batch jobscrip vsc30032  R       0:02      1 n28c30n3\n",
      "        60666074_6     batch jobscrip vsc30032  R       0:02      1 p33c30n1\n",
      "        60666074_7     batch jobscrip vsc30032  R       0:02      1 p33c30n1\n",
      "        60666074_8     batch jobscrip vsc30032  R       0:02      1 p33c30n2\n",
      "        60666074_9     batch jobscrip vsc30032  R       0:02      1 p33c32n2\n",
      "       60666074_10     batch jobscrip vsc30032  R       0:02      1 p33c32n2\n",
      "       60666074_11     batch jobscrip vsc30032  R       0:02      1 p33c32n2\n",
      "       60666074_12     batch jobscrip vsc30032  R       0:02      1 p33c32n2\n",
      "       60666074_13     batch jobscrip vsc30032  R       0:02      1 p33c32n2\n",
      "       60666074_14     batch jobscrip vsc30032  R       0:02      1 p33c32n2\n",
      "       60666074_15     batch jobscrip vsc30032  R       0:02      1 p33c32n2\n",
      "       60666074_16     batch jobscrip vsc30032  R       0:02      1 p33c32n2\n",
      "       60666074_17     batch jobscrip vsc30032  R       0:02      1 p33c32n2\n",
      "       60666074_18     batch jobscrip vsc30032  R       0:02      1 p33c32n2\n",
      "       60666074_19     batch jobscrip vsc30032  R       0:02      1 p33c32n2\n",
      "       60666074_20     batch jobscrip vsc30032  R       0:02      1 p33c32n2\n",
      "          60665918 interacti sys/dash vsc30032  R      15:57      1 k28i14\n"
     ]
    }
   ],
   "source": [
    "squeue --cluster=wice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "045d6c53-482b-4781-bcec-f3646293dc34",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLUSTER: wice\n",
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "       60666074_81     batch jobscrip vsc30032  R       0:13      1 m28c20n2\n",
      "       60666074_82     batch jobscrip vsc30032  R       0:13      1 s28c11n2\n",
      "       60666074_83     batch jobscrip vsc30032  R       0:13      1 n28c30n1\n",
      "       60666074_84     batch jobscrip vsc30032  R       0:13      1 n28c30n1\n",
      "       60666074_85     batch jobscrip vsc30032  R       0:13      1 n28c30n3\n",
      "       60666074_86     batch jobscrip vsc30032  R       0:13      1 n28c30n3\n",
      "       60666074_87     batch jobscrip vsc30032  R       0:13      1 n28c30n3\n",
      "       60666074_88     batch jobscrip vsc30032  R       0:13      1 n28c30n3\n",
      "       60666074_89     batch jobscrip vsc30032  R       0:13      1 n28c30n3\n",
      "       60666074_90     batch jobscrip vsc30032  R       0:13      1 n28c30n3\n",
      "       60666074_91     batch jobscrip vsc30032  R       0:13      1 n28c30n3\n",
      "       60666074_92     batch jobscrip vsc30032  R       0:13      1 n28c30n3\n",
      "       60666074_93     batch jobscrip vsc30032  R       0:13      1 n28c30n3\n",
      "       60666074_94     batch jobscrip vsc30032  R       0:13      1 n28c30n3\n",
      "       60666074_95     batch jobscrip vsc30032  R       0:13      1 n28c30n3\n",
      "       60666074_96     batch jobscrip vsc30032  R       0:13      1 n28c30n3\n",
      "       60666074_97     batch jobscrip vsc30032  R       0:13      1 n28c30n3\n",
      "       60666074_98     batch jobscrip vsc30032  R       0:13      1 n28c30n3\n",
      "       60666074_99     batch jobscrip vsc30032  R       0:13      1 n28c30n3\n",
      "      60666074_100     batch jobscrip vsc30032  R       0:13      1 n28c30n3\n",
      "          60665918 interacti sys/dash vsc30032  R      20:09      1 k28i14\n"
     ]
    }
   ],
   "source": [
    "squeue --cluster=wice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e778bd56-4c76-4144-994a-004ee5329d24",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLUSTER: wice\n",
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "          60665918 interacti sys/dash vsc30032  R      20:34      1 k28i14\n"
     ]
    }
   ],
   "source": [
    "squeue --cluster=wice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "618b3c31-3375-40f8-86f3-c6a1d2948371",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60665928_58,62,67-68,80,82,87,100: Job has already finished\n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "scontrol release 60665928"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f508b7b2-561b-4743-8a37-75d6bdcb0bfe",
   "metadata": {
    "tags": []
   },
   "source": [
    "When the job finishes, you will notice a lot of files, each containing the output of a single work item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "78d641b1-c36d-4c34-befd-8070775670f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slurm-60666074_100.out\tslurm-60666074_40.out  slurm-60666074_71.out\n",
      "slurm-60666074_10.out\tslurm-60666074_41.out  slurm-60666074_72.out\n",
      "slurm-60666074_11.out\tslurm-60666074_42.out  slurm-60666074_73.out\n",
      "slurm-60666074_12.out\tslurm-60666074_43.out  slurm-60666074_74.out\n",
      "slurm-60666074_13.out\tslurm-60666074_44.out  slurm-60666074_75.out\n",
      "slurm-60666074_14.out\tslurm-60666074_45.out  slurm-60666074_76.out\n",
      "slurm-60666074_15.out\tslurm-60666074_46.out  slurm-60666074_77.out\n",
      "slurm-60666074_16.out\tslurm-60666074_47.out  slurm-60666074_78.out\n",
      "slurm-60666074_17.out\tslurm-60666074_48.out  slurm-60666074_79.out\n",
      "slurm-60666074_18.out\tslurm-60666074_49.out  slurm-60666074_7.out\n",
      "slurm-60666074_19.out\tslurm-60666074_4.out   slurm-60666074_80.out\n",
      "slurm-60666074_1.out\tslurm-60666074_50.out  slurm-60666074_81.out\n",
      "slurm-60666074_20.out\tslurm-60666074_51.out  slurm-60666074_82.out\n",
      "slurm-60666074_21.out\tslurm-60666074_52.out  slurm-60666074_83.out\n",
      "slurm-60666074_22.out\tslurm-60666074_53.out  slurm-60666074_84.out\n",
      "slurm-60666074_23.out\tslurm-60666074_54.out  slurm-60666074_85.out\n",
      "slurm-60666074_24.out\tslurm-60666074_55.out  slurm-60666074_86.out\n",
      "slurm-60666074_25.out\tslurm-60666074_56.out  slurm-60666074_87.out\n",
      "slurm-60666074_26.out\tslurm-60666074_57.out  slurm-60666074_88.out\n",
      "slurm-60666074_27.out\tslurm-60666074_58.out  slurm-60666074_89.out\n",
      "slurm-60666074_28.out\tslurm-60666074_59.out  slurm-60666074_8.out\n",
      "slurm-60666074_29.out\tslurm-60666074_5.out   slurm-60666074_90.out\n",
      "slurm-60666074_2.out\tslurm-60666074_60.out  slurm-60666074_91.out\n",
      "slurm-60666074_30.out\tslurm-60666074_61.out  slurm-60666074_92.out\n",
      "slurm-60666074_31.out\tslurm-60666074_62.out  slurm-60666074_93.out\n",
      "slurm-60666074_32.out\tslurm-60666074_63.out  slurm-60666074_94.out\n",
      "slurm-60666074_33.out\tslurm-60666074_64.out  slurm-60666074_95.out\n",
      "slurm-60666074_34.out\tslurm-60666074_65.out  slurm-60666074_96.out\n",
      "slurm-60666074_35.out\tslurm-60666074_66.out  slurm-60666074_97.out\n",
      "slurm-60666074_36.out\tslurm-60666074_67.out  slurm-60666074_98.out\n",
      "slurm-60666074_37.out\tslurm-60666074_68.out  slurm-60666074_99.out\n",
      "slurm-60666074_38.out\tslurm-60666074_69.out  slurm-60666074_9.out\n",
      "slurm-60666074_39.out\tslurm-60666074_6.out\n",
      "slurm-60666074_3.out\tslurm-60666074_70.out\n"
     ]
    }
   ],
   "source": [
    "ls slurm-*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "72eba3b9-7d6a-4bf8-88be-8a9214a99f65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLURM_JOB_ID: 60666074\n",
      "SLURM_JOB_USER: vsc30032\n",
      "SLURM_JOB_ACCOUNT: lpt2_sysadmin\n",
      "SLURM_JOB_NAME: jobscript_parallel.slurm\n",
      "SLURM_CLUSTER_NAME: wice\n",
      "SLURM_JOB_PARTITION: batch\n",
      "SLURM_ARRAY_JOB_ID: 60666074\n",
      "SLURM_ARRAY_TASK_ID: 100\n",
      "SLURM_NNODES: 1\n",
      "SLURM_NODELIST: n28c30n3\n",
      "SLURM_JOB_CPUS_PER_NODE: 1\n",
      "Date: Thu Aug 31 11:51:21 CEST 2023\n",
      "Walltime: 00-00:05:00\n",
      "========================================================================\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "cat slurm-*_100.out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e104815-1d86-42c6-b071-d62f4754449c",
   "metadata": {},
   "source": [
    "Indeed, work item 100 would be the multiplication of the values 10 and 10.  It is the last line in `data.csv` and hence your last work item.\n",
    "\n",
    "Since this is just a tutorial job, you probably would like to remove these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ee4b1755-db84-439f-819a-7312a8edbaba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rm slurm-*.out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27760f7e-47a2-4bd4-8edf-d771051da1f7",
   "metadata": {},
   "source": [
    "### Log file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a8c321-6d56-4532-92fd-7da90ed1d2e9",
   "metadata": {},
   "source": [
    "You will remember that you enabled logging using the `alog` commands in `jobscript_parallel.slurm`.  This has resulted in a file that contains information about the work items' execution:\n",
    "\n",
    "  1. its number\n",
    "  1. when they started,\n",
    "  1. the name of the compute node the work item was executed on,\n",
    "  1. when they completed, and\n",
    "  1. the exit status, if there was a failure.\n",
    "  \n",
    "The name of this job script is the name of your job, with `.log` appended, followed by the job ID.  The command below shows you the first 20 lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "10f5af0a-41ec-4bed-a51b-3767f762795a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> jobscript_parallel.slurm.log60665928 <==\n",
      "2 started by s28c11n2 at 2023-08-31 11:37:26\n",
      "2 completed by s28c11n2 at 2023-08-31 11:37:27\n",
      "26 started by n28c30n3 at 2023-08-31 11:37:27\n",
      "26 completed by n28c30n3 at 2023-08-31 11:37:27\n",
      "1 started by m28c20n2 at 2023-08-31 11:37:27\n",
      "1 completed by m28c20n2 at 2023-08-31 11:37:27\n",
      "9 started by n28c30n1 at 2023-08-31 11:37:27\n",
      "20 started by n28c30n1 at 2023-08-31 11:37:27\n",
      "9 completed by n28c30n1 at 2023-08-31 11:37:27\n",
      "20 completed by n28c30n1 at 2023-08-31 11:37:27\n",
      "40 started by p33c20n1 at 2023-08-31 11:37:27\n",
      "86 started by p33c20n1 at 2023-08-31 11:37:27\n",
      "40 completed by p33c20n1 at 2023-08-31 11:37:27\n",
      "10 started by n28c30n1 at 2023-08-31 11:37:27\n",
      "86 completed by p33c20n1 at 2023-08-31 11:37:28\n",
      "10 completed by n28c30n1 at 2023-08-31 11:37:28\n",
      "37 started by p33c20n1 at 2023-08-31 11:37:28\n",
      "37 completed by p33c20n1 at 2023-08-31 11:37:28\n",
      "14 started by n28c30n1 at 2023-08-31 11:37:28\n",
      "88 started by p33c30n1 at 2023-08-31 11:37:28\n",
      "\n",
      "==> jobscript_parallel.slurm.log60666074 <==\n",
      "2 started by s28c11n2 at 2023-08-31 11:47:04\n",
      "5 started by n28c30n3 at 2023-08-31 11:47:04\n",
      "7 started by p33c30n1 at 2023-08-31 11:47:04\n",
      "8 started by p33c30n2 at 2023-08-31 11:47:04\n",
      "4 started by n28c30n1 at 2023-08-31 11:47:04\n",
      "1 started by m28c20n2 at 2023-08-31 11:47:05\n",
      "17 started by p33c32n2 at 2023-08-31 11:47:07\n",
      "15 started by p33c32n2 at 2023-08-31 11:47:07\n",
      "19 started by p33c32n2 at 2023-08-31 11:47:07\n",
      "3 started by n28c30n1 at 2023-08-31 11:47:07\n",
      "6 started by p33c30n1 at 2023-08-31 11:47:08\n",
      "10 started by p33c32n2 at 2023-08-31 11:47:07\n",
      "18 started by p33c32n2 at 2023-08-31 11:47:08\n",
      "12 started by p33c32n2 at 2023-08-31 11:47:08\n",
      "16 started by p33c32n2 at 2023-08-31 11:47:09\n",
      "14 started by p33c32n2 at 2023-08-31 11:47:09\n",
      "11 started by p33c32n2 at 2023-08-31 11:47:10\n",
      "9 started by p33c32n2 at 2023-08-31 11:47:12\n",
      "2 completed by s28c11n2 at 2023-08-31 11:47:14\n",
      "5 completed by n28c30n3 at 2023-08-31 11:47:14\n"
     ]
    }
   ],
   "source": [
    "head -20 jobscript_parallel.slurm.log*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b905c0-1017-43e9-a527-31e361f7a8b7",
   "metadata": {},
   "source": [
    "You could of course eyeball this file to determine whether all work items completed succesfully, but atools has a command to simplify that considerably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "457ad20a-08c7-4233-aff4-27982378f796",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "  items completed: 100\n",
      "  items failed: 0\n",
      "  items to do: 0\n"
     ]
    }
   ],
   "source": [
    "arange  --data data.csv  --log jobscript_parallel.slurm.log*  --summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b4b563-4a28-4567-9ecc-b2c3515438e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash (ipykernel)",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
