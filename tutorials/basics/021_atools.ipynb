{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cf14d57-ac2c-4ecd-a200-e4d907c29333",
   "metadata": {},
   "source": [
    "# HPC intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af640881-f322-4287-a6f4-ef65e0ee91cf",
   "metadata": {},
   "source": [
    "## atools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ac4663-1aa8-4c0c-8490-0df632a54d72",
   "metadata": {},
   "source": [
    "There are many situations in which you want to run an application for (potentially many) different input parameters.  These parameters can be command line options you run your application with, or file names you provide and so on.\n",
    "\n",
    "Of course, you could submit a job for each of the instances of your problem, but that would result in many jobs.  Moreover, quite some bookkeeping would be required if some instances fail, while others succeed.  You typically don't have a convenient way to get an overview of which instances failed, and hence have to be redone.\n",
    "\n",
    "Alternatively, you could simply do all these instances looping over all the parameters.  This would result in potentially prohibitively long run times, and, more importantly, you would not be exploiting a supercomputers main feature: executing work in parallel.\n",
    "\n",
    "atools has been designed to make it easy for you to run many instances of a problem in parallel, and it takes care of the bookkeeping for you as well.  An instance of the problem that you want to compute is called a *work item* in the context of atools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3be777-a69b-47f8-a329-facbec0330f2",
   "metadata": {},
   "source": [
    "### Job script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49de7a44-8062-4bbb-bf8e-e2362f27ba17",
   "metadata": {},
   "source": [
    "The first step is to make a few modifications to your job script.  By way of example, use a script that simply calculates and displays the product of two numbers that you also used in the [tutorial on jobs](020_jobs.ipynb).\n",
    "\n",
    "```bash\n",
    "#!/usr/bin/env bash\n",
    "#SBATCH --accoun=lp_multiscale_physics\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=1g\n",
    "#SBATCH --time=00:05:00\n",
    "\n",
    "# actual computation, a bit boring\n",
    "for i in $(seq 1 10)\n",
    "do\n",
    "    for j in $(seq 1 10)\n",
    "    do\n",
    "        echo $(( $i * $j ))\n",
    "    done\n",
    "done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda8376c-c321-4c94-b67e-a633203f8a49",
   "metadata": {},
   "source": [
    "In this job script, you do all computations sequentially, but to speed things up, you would like to do them in parallel as independent jobs.  So you can rewrite the job script such that it only does a single multiplication.\n",
    "\n",
    "```bash\n",
    "#!/usr/bin/env bash\n",
    "#SBATCH --accoun=lp_multiscale_physics\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=1g\n",
    "#SBATCH --time=00:05:00\n",
    "\n",
    "# actual computation, a bit boring\n",
    "echo $(( $i * $j ))\n",
    "```\n",
    "\n",
    "The job script has been adapted to compute a single work item."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d2603b-1b9c-41cd-8e04-55ce27482b50",
   "metadata": {},
   "source": [
    "This is where atools comes in.  You can make a few more modifications to this job script to use it.  The values of `i` and `j` will be read from a Comma Separated Value file (CSV file).\n",
    "\n",
    "The first line of this file lists the names of the variables, each line after that the values that correspond to the work items.  So for this example, that would look as follows.  \n",
    "\n",
    "```\n",
    "i,j\n",
    "1,1\n",
    "1,2\n",
    "1,3\n",
    "...\n",
    "10,8\n",
    "10,9\n",
    "10,10\n",
    "```\n",
    "\n",
    "You don't have to type all that, there is a data file `data.csv` available that you can copy.  You can find it in the `021_artefacts` directory."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d44421c-e833-455a-bf02-e810952a235c",
   "metadata": {},
   "source": [
    "As it is, this script would fail since at this point the variables `i` and `j` are not defined.  You have to make sure that atools can do its magic.  For that purpose, you have to make a few more modifications to the job script.\n",
    "\n",
    "1. Load the `atools` module.\n",
    "2. Log the start of the work item.\n",
    "3. Make sure that the variables used in the script are initialized.\n",
    "4. Log the end of the work item.\n",
    "\n",
    "```bash\n",
    "#!/usr/bin/env bash\n",
    "#SBATCH --accoun=lp_multiscale_physics\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=1g\n",
    "#SBATCH --time=00:05:00\n",
    "\n",
    "# make sure the module system starts from a clean slate and load the atools module\n",
    "module purge\n",
    "module load atools\n",
    "\n",
    "# log the start of the work item\n",
    "alog  --state start\n",
    "\n",
    "# initialize the variables\n",
    "source <(aenv --data data.csv)\n",
    "\n",
    "# actual computation, a bit boring\n",
    "echo $(( $i * $j ))\n",
    "\n",
    "# log the end of the work item\n",
    "alog  --state end  --exit $?\n",
    "```\n",
    "\n",
    "Now your job script is fully adapted to use atools features.  It is available in the `021_artefacts` directory as `jobscript_parallel.slurm`.   Don't forget to change the credit account name to the one you have access to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1840b718-5f86-4504-9b87-73fca85f2280",
   "metadata": {},
   "source": [
    "### Job submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086d359c-9dba-49e7-aa06-f7e41df1e5f0",
   "metadata": {},
   "source": [
    "You can submit an atools job almost the same way as an ordinary job, except that you need to specify the `--array` option for `sbatch`.  If you know the number of work items, 100 in the `data.csv` file you are using, you can simply use `--array=1-100`.  Otherwise, atools can help you determine it easily.\n",
    "\n",
    "First, load the atools module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5e1812-e512-4db9-a258-785975dd2901",
   "metadata": {},
   "outputs": [],
   "source": [
    "module load atools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bf6464-1e39-4db5-ab9e-b2fbcdef1a95",
   "metadata": {},
   "source": [
    "Next, submit the job as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daf511c-8063-40d4-9c3c-ef09eb84b0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbatch  --array=$(arange --data data.csv)  jobscript_parallel.slurm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
